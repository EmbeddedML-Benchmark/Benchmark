{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Imports*\n",
    "\n",
    "Install necessary packages for emotional classifer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Embedding, LSTM, Dropout, Lambda, Layer\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Data Preprocessing & Splitting*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word-based tokenization:\n",
      "X_train shape: 4935\n",
      "X_test shape: 1234\n",
      "Y_train shape: 4935\n",
      "\n",
      "BERT-based tokenization:\n",
      "X_train shape: (4935, 50)\n",
      "X_test shape: (1234, 50)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"EmotionDataset/emotions_dataset.csv\")\n",
    "\n",
    "word_tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "word_tokenizer.fit_on_texts(df['sentence'])\n",
    "\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-fa-base-uncased\")\n",
    "\n",
    "sentences = df['sentence'].tolist()\n",
    "labels = pd.factorize(df['emotion'])[0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(sentences, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "X_train_word_tokens = word_tokenizer.texts_to_sequences(X_train)\n",
    "X_test_word_tokens = word_tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_bert_tokens = bert_tokenizer(X_train, padding=\"max_length\", truncation=True, max_length=50, return_tensors=\"tf\")\n",
    "X_test_bert_tokens = bert_tokenizer(X_test, padding=\"max_length\", truncation=True, max_length=50, return_tensors=\"tf\")\n",
    "\n",
    "with open(\"word_tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(word_tokenizer, f)\n",
    "\n",
    "with open(\"bert_tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(bert_tokenizer, f)\n",
    "\n",
    "print(\"Word-based tokenization:\")\n",
    "print(\"X_train shape:\", len(X_train_word_tokens))\n",
    "print(\"Y_train shape:\", len(y_train))\n",
    "print(\"X_test shape:\", len(X_test_word_tokens))\n",
    "\n",
    "print(\"\\nBERT-based tokenization:\")\n",
    "print(\"X_train shape:\", X_train_bert_tokens['input_ids'].shape)\n",
    "print(\"X_test shape:\", X_test_bert_tokens['input_ids'].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Training High-Parameter Model (BERT-based)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ROG\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ROG\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at HooshvareLab/bert-fa-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at HooshvareLab/bert-fa-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"HooshvareLab/bert-fa-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "bert_model = TFAutoModel.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train input_ids shape: (4935, 50)\n",
      "X_train attention_mask shape: (4935, 50)\n",
      "y_train shape: 4935\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 50\n",
    "\n",
    "print(\"X_train input_ids shape:\", X_train_bert_tokens['input_ids'].shape)\n",
    "print(\"X_train attention_mask shape:\", X_train_bert_tokens['attention_mask'].shape)\n",
    "print(\"y_train shape:\", len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ROG\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:222: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class BertLayer(Layer):\n",
    "    def __init__(self, bert_model, **kwargs):\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "        self.bert_model = bert_model\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_ids, attention_mask = inputs\n",
    "        bert_output = self.bert_model(input_ids, attention_mask=attention_mask)\n",
    "        return bert_output.pooler_output  # Extract CLS token output\n",
    "\n",
    "input_ids = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_ids\")\n",
    "attention_mask = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "bert_output = BertLayer(bert_model)([input_ids, attention_mask])\n",
    "\n",
    "dense = Dense(128, activation='relu')(bert_output)\n",
    "output = Dense(len(set(labels)), activation='softmax')(dense)\n",
    "\n",
    "model_parsbert = Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "\n",
    "model_parsbert.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m 79/309\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:11\u001b[0m 313ms/step - accuracy: 0.3852 - loss: 1.9531"
     ]
    }
   ],
   "source": [
    "model_parsbert.fit(\n",
    "    x=[X_train_bert_tokens['input_ids'], X_train_bert_tokens['attention_mask']], \n",
    "    y=np.array(y_train),  # Ensure y_train is a NumPy array\n",
    "    epochs=5, batch_size=16, \n",
    "    validation_data=([X_test_bert_tokens['input_ids'], X_test_bert_tokens['attention_mask']], np.array(y_test))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Convert BERT Model to TFLite*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_parsbert)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_parsbert = converter.convert()\n",
    "\n",
    "with open(\"Models/parsbert_emotion_model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_parsbert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Training Lightweight Model (LSTM)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=64, input_length=50),\n",
    "    LSTM(32, return_sequences=True),\n",
    "    LSTM(16),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(len(set(labels)), activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model_lstm.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Convert LSTM Model to TFLite*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_lstm)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model_lstm = converter.convert()\n",
    "with open(\"Models/lstm_emotion_model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Test & Evaluate Models*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model_lstm.predict(X_test), axis=1)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=df['emotion'].unique(), yticklabels=df['emotion'].unique())\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix for LSTM Model\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
